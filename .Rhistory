knitr::opts_chunk$set(echo = TRUE)
df_norm <- scale(df[,4:11])
df
# load data
df <- read.csv("~/Documents/SPRING2023/Capstone/plantbasedpredictions/clean.csv")
df <- na.omit(df)
df
# Wilkes-Shapiro Test for Normality
shapiro.test(df[,4])
shapiro.test(df[,5])
shapiro.test(df[,6])
shapiro.test(df[,7])
shapiro.test(df[,8])
shapiro.test(df[,9])
shapiro.test(df[,10])
shapiro.test(df[,11])
predictions <- predict(logistic, testX, type = "response")
df_norm <- scale(df[,4:11])
knitr::opts_chunk$set(echo = TRUE)
# load data
df <- read.csv("~/Documents/SPRING2023/Capstone/plantbasedpredictions/clean.csv")
df <- na.omit(df)
library(caret)
set.seed(0)
# logistic regression
(mean(df$MEATALTPCT))
df$likelyYes <- ifelse(df$MEATALTPCT > mean(df$MEATALTPCT), 1, 0)
train_set <- createDataPartition(df$likelyYes, p = .8, list = FALSE)
train <- df[train_set,]
test <- df[-train_set,]
logistic <- glm(likelyYes ~ URBANPROP+MEDHHINC+ POVPROP+FOODCPI+LAPOPHALF+LAPOP1+FOODHOME, data = train, family = "binomial")
testX <- test[,c("URBANPROP","MEDHHINC", "POVPROP","FOODCPI","LAPOPHALF","LAPOP1","FOODHOME")]
testY <- test[,"likelyYes"]
predictions <- predict(logistic, testX, type = "response")
predictions$class <- ifelse(predictions > 0.5, 1, 0)
confusionMatrix(as.factor(predictions$class), as.factor(testY))
summary(logistic)
# set up cross validation for logistic regression and SVM
library(e1071)
# tune svm on training dataset
set.seed(0)
tune.out=
tune(svm,likelyYes ~ URBANPROP + MEDHHINC + POVPROP + FOODCPI +
LAPOPHALF + LAPOP1 + FOODHOME,data=train , kernel = "linear", ranges
=list(cost=c(0.001,0.01,0.1, 1,5,10,100)))
summary(tune.out)
bestmod <- tune.out$best.model
summary(bestmod)
predictions1 <- predict(bestmod, testX, type = "response")
predictions1$class <- ifelse(predictions1 > 0.5, 1, 0)
confusionMatrix(as.factor(predictions1$class), as.factor(testY))
